以下是该脚本的详细使用文档：

---

# 使用文档：训练语言模型并保存最佳检查点

本文档提供了运行该脚本的指南，包括环境设置、参数配置以及运行说明。

## 1. 环境需求

### 平台和环境

- **平台要求**：建议在 [autodl.com](https://autodl.com) 平台的 **L20 区域**下创建项目。
- **镜像要求**：选择并拉取名称为 `SFT` 的镜像。

### 安装依赖包

在运行该脚本之前，确保安装以下 Python 库：

```bash
pip install transformers datasets torch flash_attn trl
```

## 2. 脚本功能

该脚本用于在指定数据集上使用 `SFTTrainer` 继续预训练语言模型，并在每个 `epoch` 后评估模型表现，保存表现最好的检查点。

## 3. 命令行参数

该脚本支持以下命令行参数：

- `--model_name`: **必需**，模型路径或模型名称。
- `--output_dir`: **必需**，保存模型输出的路径。

**示例命令**：

```bash
python train_and_save.py --model_name /root/autodl-fs/qwen/Qwen1___5-0___5B-Chat --output_dir /root/autodl-fs/qwen_0.5b_GaLore
```

## 4. 主要配置与参数说明

### 注意力机制和计算类型

- **计算类型**：`torch.bfloat16`，使用 bfloat16 类型提高计算效率。
- **注意力机制**：`flash_attention_2`，显著优化注意力计算的速度和显存占用。

### 数据集加载

加载自定义数据集 `yimeng521/sunny_datasets`，用于训练和评估。

### 分词器配置

加载分词器，并设定 `pad_token` 为 `eos_token`，方便处理序列填充操作。

### 模型加载与配置

- **模型**：从 `model_name` 路径加载 `AutoModelForCausalLM`。
- **设备映射**：`device_map="auto"`，自动分配设备。
- **梯度检查点**：启用梯度检查点，降低显存占用。
- **禁用缓存**：模型配置中禁用缓存，以确保更有效的训练。

### 训练参数 `training_arguments`

- `output_dir`: 模型保存路径。
- `evaluation_strategy`: 在每个 `epoch` 结束后进行评估。
- `save_strategy`: 在每个 `epoch` 结束后保存模型检查点。
- `do_eval`: 启用评估。
- **优化器**：设置优化器为 `galore_adamw`，并设置特定参数：
  - `optim_args`: 设置 `rank=1024, update_proj_gap=500, scale=1.8`。
  - `optim_target_modules`: 仅优化注意力和 MLP 模块。
- `learning_rate`: 学习率，设置为 `1e-5`。
- `bf16`: 自动检测 GPU 是否支持 bfloat16 数据类型。
- `num_train_epochs`: 训练轮数，设置为 `10`。
- `warmup_ratio`: 学习率预热比例 `0.1`。
- `lr_scheduler_type`: 学习率调度器类型，设置为 `linear`。
- `logging_steps`: 日志记录步长，设置为 `85`。
- **保存策略**：
  - `load_best_model_at_end`: 设置为 `False`，在训练结束后不自动加载最佳模型。
  - `save_total_limit`: 保存最新的 1 个模型检查点。

### 自定义回调 `BestModelCheckpointCallback`

该回调在每次评估后记录最低 `eval_loss` 的检查点路径，确保训练结束后可以加载并保存表现最佳的模型。

## 5. 执行流程

1. **解析命令行参数**：从命令行获取 `model_name` 和 `output_dir` 参数。
2. **安装依赖库**：使用 `os.system` 命令自动安装 `flash_attention` 库。
3. **初始化分词器**：加载分词器并设置填充方式。
4. **加载数据集**：使用 `yimeng521/sunny_datasets` 数据集，分为训练集和测试集。
5. **加载模型**：指定 `bfloat16` 类型和 `flash_attention` 注意力实现方式。
6. **配置训练参数**：使用 `TrainingArguments` 设置训练相关配置。
7. **自定义回调**：实现 `BestModelCheckpointCallback` 用于记录最佳检查点。
8. **创建训练器 `SFTTrainer`**：初始化 `SFTTrainer`，指定训练数据集和评估数据集。
9. **开始训练**：调用 `trainer.train()` 开始训练。
10. **保存最佳检查点**：在训练结束后，加载并保存最低 `eval_loss` 的最佳检查点。

## 6. 运行脚本

将该脚本保存为 `train_and_save.py`，并通过命令行执行：

```bash
python train_and_save.py --model_name /root/autodl-fs/qwen/Qwen1___5-0___5B-Chat --output_dir /root/autodl-fs/qwen_0.5b_GaLore
```

## 7. 结果输出

脚本完成后，将输出以下内容：

- 训练过程中每个 `epoch` 的日志记录。
- 评估中最低 `eval_loss` 的模型检查点路径。
- 最佳检查点的 `model` 和 `tokenizer` 将保存到 `output_dir`。