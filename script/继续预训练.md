已将您提供的要求更新到文档中，以下是更新后的版本：

---

# 继续预训练脚本使用文档

本文档提供了对继续预训练脚本的使用说明和各项参数配置的解释。

## 1. 环境需求

在运行该脚本之前，请确保：

- **平台要求**：使用 [autodl.com](https://autodl.com) 平台，并在 **L20 区域**下创建项目。
- **镜像要求**：选择并拉取名称为 `continue_pre_train` 的镜像。
- **安装包**：需要以下 Python 库：

  ```bash
  pip install transformers datasets torch
  ```

## 2. 参数说明

### 模型和数据集相关参数

- `model_checkpoint`: 模型检查点路径，用于加载预训练模型。此处为自定义路径 `/root/autodl-fs/Qwen/Qwen2___5-32B-Instruct`。
- `tokenizer_checkpoint`: 分词器的检查点路径。与模型路径一致，通常建议保持一致。
- `datasets`: 加载的数据集，此处为 `"soikit/class_stand"`。可根据需求替换成其他数据集。

### 数据预处理相关

1. **分词函数 `tokenize_function`**：
   将文本数据进行分词，便于后续模型处理。分词器从 `tokenizer_checkpoint` 加载。

2. **文本分组函数 `group_texts`**：
   - 该函数将所有文本拼接，并以 `block_size` 为步长对其进行切割。
   - `block_size`：分组时的块大小。默认为 1024，但可根据硬件条件和任务需求调整。
   - `labels`: 生成的标签与输入 `input_ids` 相同，用于监督学习。

3. **Tokenized Dataset**：
   - `tokenized_datasets`：对原始数据集进行分词处理。
   - `lm_datasets`：将分词后的数据集进一步按块大小分组。

### 训练参数 `training_args`

训练参数由 `TrainingArguments` 进行配置，主要参数解释如下：

- `output_dir`: 模型保存路径，默认为`/root/autodl-tmp`。
- `evaluation_strategy`: 评估策略，每个 `epoch` 后进行一次评估。
- `save_strategy`: 保存策略，每个 `epoch` 后保存一次模型。
- `num_train_epochs`: 训练轮数，默认为300。
- `learning_rate`: 学习率，设置为`2e-5`。
- `weight_decay`: 权重衰减系数。
- `save_total_limit`: 限制最多保存的模型数量，此处为1。
- `load_best_model_at_end`: 在训练结束时加载表现最佳的模型。
- `metric_for_best_model`: 用于选择最佳模型的指标，此处为 `eval_loss`。
- `greater_is_better`: 当 `metric_for_best_model` 值越小越好时设置为 `False`。
- **降低显存占用设置**：
  - `per_device_train_batch_size`: 每个设备上的训练批次大小。
  - `per_device_eval_batch_size`: 每个设备上的评估批次大小。
  - `gradient_accumulation_steps`: 梯度累积步数，有效批次大小 = 批次大小 × 累积步数。
  - `fp16`: 启用混合精度，适用于支持该特性的显卡。
  - `optim`: 优化器类型，设置为 `adamw_torch` 以节省显存。
- `logging_steps`: 每隔多少步记录日志。
- `dataloader_num_workers`: 数据加载线程数，根据CPU和显存情况适当调整。

### 显卡数量要求

根据模型大小配置显卡数量,显卡内存24gb。

- **小模型（如 < 6B 参数）**：推荐使用 1-2 张显卡。
- **中等模型（如 6B - 13B 参数）**：建议使用 4 张显卡。
- **大模型（如 > 13B 参数）**：建议使用 8 张或更多显卡，以保证显存充足，避免OOM错误。

## 3. 脚本执行流程

1. **初始化分词器**：加载指定路径的分词器。
2. **数据加载和预处理**：
   - 使用 `load_dataset` 函数加载数据集。
   - 将文本数据分词并按块大小进行分组。
3. **加载模型**：从 `model_checkpoint` 路径加载模型。
4. **初始化 Trainer**：配置 `Trainer`，并指定训练数据集和评估数据集。
5. **训练模型**：调用 `trainer.train()` 开始训练。
6. **评估模型**：训练完成后，使用 `trainer.evaluate()` 对模型进行评估并输出困惑度（Perplexity）。

## 4. 运行脚本

将该脚本保存为 `train_script.py`，然后通过命令行执行：

```bash
python continue_train_script.py
```

## 5. 结果

脚本完成后，会在控制台输出困惑度 (Perplexity)，并在 `output_dir` 指定的目录下保存训练好的模型。


## 6.附注：
在`continue_pre_train`的镜像中，还有一些ipynb文件中是继续预训练代码和对继续预训练结束后保存的模型进行再加载，然后推理的代码。 这些代码都是做测试用，可以参考。
另外，如果你会Llamafactory，也可以拉取`llamafactory_train`镜像，llamafactory的使用请参考：https://github.com/hiyouga/LLaMA-Factory  
llamafactory包含了全面的流程，从继续预训练，SFT,Lora,
